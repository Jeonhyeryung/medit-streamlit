import streamlit as st
import openai
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.chat_models import ChatOpenAI
from config import AI_CONFIG
import base64
import asyncio

def ai_service():
    # Initialize session state
    if "retriever" not in st.session_state:
        st.session_state.retriever = None
    if "pdf_file" not in st.session_state:
        st.session_state.pdf_file = None
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}]
    if "ai_service_option" not in st.session_state:
        st.session_state.ai_service_option = False
    if "markdown_document" not in st.session_state:
        st.session_state.markdown_document = ""

    col1, col2 = st.columns(2)
    with col1:
        toggle = st.toggle("ë¬¸ì„œ ìƒì„± ê¸°ëŠ¥ ì‚¬ìš©", key="toggle_document_generation", help="Use Document Generation")

    if toggle:
        st.session_state.ai_service_option = True
    else:
        st.session_state.ai_service_option = False

    if st.session_state.ai_service_option:
        st.write("ë¬¸ì„œ ìƒì„± ê¸°ëŠ¥ í˜ì´ì§€ì…ë‹ˆë‹¤.")

        if st.session_state.markdown_document:
            col1, col2 = st.columns([2, 3])

            with col1:
                st.header("ğŸ’¬ Chatbot")
                for msg in st.session_state.messages:
                    st.chat_message(msg["role"]).write(msg["content"])

                if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
                    openai_api_key = AI_CONFIG["openai"]["api_key"]
                    if not openai_api_key:
                        st.info("Please add your OpenAI API key to continue.")
                        st.stop()

                    st.session_state.messages.append({"role": "user", "content": prompt})
                    st.chat_message("user").write(prompt)

                    # LLM ì„¤ì •
                    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, openai_api_key=openai_api_key)
                    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
                    template = """ì‚¬ìš©ìì˜ ìš”ì²­ ì‚¬í•­ì— ë”°ë¼ ì•„ë˜ ë¬¸ì„œë¥¼ ìˆ˜ì •í•˜ì‹œì˜¤.
                    ì˜¤ì§ ìˆ˜ì •ëœ ë¬¸ì„œ ê²°ê³¼ë§Œ outputìœ¼ë¡œ ì œê³µí•˜ì‹œì˜¤.
                    {context}
                    ì§ˆë¬¸: {question}
                    ìˆ˜ì •ëœ ë¬¸ì„œ:"""
                    rag_prompt_custom = PromptTemplate.from_template(template)

                    # RAG chain ì„¤ì •
                    rag_chain = {"context": RunnablePassthrough(lambda: st.session_state.markdown_document), "question": RunnablePassthrough()} | rag_prompt_custom | llm
                    response = rag_chain.invoke(f'{prompt}')
                    msg = response.content

                    st.session_state.messages.append({"role": "assistant", "content": msg})
                    st.chat_message("assistant").write(msg)

                    # Update markdown document based on chatbot response
                    st.session_state.markdown_document = msg

            with col2:
                st.header("ğŸ“„ PDF Viewer")
                markdown_document = st.session_state.markdown_document

                st.markdown(markdown_document)
                st.download_button("Download Updated Markdown", markdown_document, file_name="updated_output.md")

                with open("updated_output.md", "wb") as f:
                    f.write(markdown_document.encode('utf-8'))
                with open("updated_output.md", "rb") as f:
                    base64_pdf = base64.b64encode(f.read()).decode('utf-8')
                pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}#toolbar=0&navpanes=0&scrollbar=0" width="100%" height="500" type="application/pdf"></iframe>'
                st.markdown(pdf_display, unsafe_allow_html=True)
    else:
        col1, col2 = st.columns([3, 2])

        with col1:
            st.header("ğŸ“„ PDF ì—…ë¡œë“œ")
            uploaded_file = st.file_uploader("Upload a PDF file", type=("pdf"))
            process = st.button("PDF ì²˜ë¦¬í•˜ê¸°")

            if process and uploaded_file:
                openai_api_key = AI_CONFIG["openai"]["api_key"]
                openai.api_key = openai_api_key

                # Save the uploaded file
                file_name = uploaded_file.name
                with open(file_name, "wb") as file:
                    file.write(uploaded_file.getvalue())

                # Store the file name in session state
                st.session_state.pdf_file = file_name

                # Load and split PDF
                loader = PyPDFLoader(file_name)
                pages = loader.load_and_split()

                # Split text into chunks
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
                splits = text_splitter.split_documents(pages)

                # Set embedding model
                embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)

                # Set up ChromaDB and retriever
                vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)
                vectorstore_retriever = vectorstore.as_retriever()

                st.session_state.retriever = vectorstore_retriever
                st.success("PDF íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # Display the PDF viewer if a file has been processed
        if st.session_state.pdf_file:
            with open(st.session_state.pdf_file, "rb") as f:
                base64_pdf = base64.b64encode(f.read()).decode('utf-8')
            pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}#toolbar=0&navpanes=0&scrollbar=0" width="100%" height="500" type="application/pdf"></iframe>'
            col1.markdown(pdf_display, unsafe_allow_html=True)

        with col2:
            st.header("ğŸ’¬ Chatbot")
            if "messages" not in st.session_state:
                st.session_state["messages"] = [{"role": "assistant", "content": "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}]

            for msg in st.session_state.messages:
                st.chat_message(msg["role"]).write(msg["content"])

            if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
                openai_api_key = AI_CONFIG["openai"]["api_key"]
                if not openai_api_key:
                    st.info("Please add your OpenAI API key to continue.")
                    st.stop()

                st.session_state.messages.append({"role": "user", "content": prompt})
                st.chat_message("user").write(prompt)

                # LLM ì„¤ì •
                llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, openai_api_key=openai_api_key)
                # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
                template = """ë‹¤ìŒê³¼ ê°™ì€ ë§¥ë½ì„ ì‚¬ìš©í•˜ì—¬ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ì‹­ì‹œì˜¤.
                ë§Œì•½ ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³ ë§Œ ë§í•˜ê³  ë‹µì„ ì§€ì–´ë‚´ë ¤ê³  í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
                ë‹µë³€ì€ ìµœëŒ€ ì„¸ ë¬¸ì¥ìœ¼ë¡œ í•˜ê³  ê°€ëŠ¥í•œ í•œ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì‹­ì‹œì˜¤.
                í•­ìƒ 'ì§ˆë¬¸í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!'ë¼ê³  ë‹µë³€ ëì— ë§í•˜ì‹­ì‹œì˜¤.
                {context}
                ì§ˆë¬¸: {question}
                ë„ì›€ì´ ë˜ëŠ” ë‹µë³€:"""
                rag_prompt_custom = PromptTemplate.from_template(template)

                # RAG chain ì„¤ì •
                rag_chain = {"context": st.session_state.retriever, "question": RunnablePassthrough()} | rag_prompt_custom | llm
                response = rag_chain.invoke(f'{prompt}')
                msg = response.content
                st.session_state.messages.append({"role": "assistant", "content": msg})
                st.chat_message("assistant").write(msg)